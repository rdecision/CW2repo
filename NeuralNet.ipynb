{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b32f93-4269-4621-8f90-58eb5baba0d3",
   "metadata": {},
   "source": [
    "## Neural Network implementation\n",
    "In this notebook, we will implement a neural network with one hidden layer from scratch. It will be used for a regression task. To test it, we will try to train the network to predict the function $y=\\sin(2 \\pi x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6267374-9f5b-4382-b509-a02a8b5121dc",
   "metadata": {},
   "source": [
    "First we load the required libraries. We'll use `numpy` and `matplotlib`. The neural net is fully implemented in `numpy`, while `matplotlib` is used for visualisation. We also import `AERO40041` which provides some testing functions that provide feedback on your implementation. After runnig certain cells, you will see \"Passed!\" if your code is working as expected, or a message to check it if it doesn't seem to be working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31bb0bd2-6a0f-4831-823c-5a4b6396f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import AERO40041"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc144c12-0df0-465a-80f6-f265135253bc",
   "metadata": {},
   "source": [
    "### Parameter matricies setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26092182-1aa5-4ab5-96e6-dbe703bc789b",
   "metadata": {},
   "source": [
    "The parameters of our network are $\\mathbf{W}^{[1]}$, $\\mathbf{W}^{[2]}$, $\\mathbf{b}^{[1]}$ and $\\mathbf{b}^{[2]}$. We define a function that initialises these to the correct sizes based on the number of neurons in each layer. The weights are initialised to some small random number. This random initialisation breaks the symmetry. If all weights were initialized to the same value (e.g., zero), the neurons in a layer would be computing the same thing, leading to a redundancy in the network that prevents effective learning.\n",
    "\n",
    "Each weight is initialised to a small random number. There are more sophisticated methods of choosing initial weights than this, but for the sake of simplicity, this will do fine. \n",
    "\n",
    "The size and shapes of $\\mathbf{W}$ and $\\mathbf{b}$ are as follows:\n",
    "\n",
    "$$\\mathbf{W}^{[1]} \\equiv \n",
    "\\begin{bmatrix}\n",
    "w_{1,1}^{[1]} & w_{1,2}^{[1]} & \\cdots & w_{1,R^{[0]}}^{[1]}\\\\\n",
    "w_{2,1}^{[1]} & w_{2,2}^{[1]} & \\cdots & w_{2,R^{[0]}}^{[1]}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{R^{[1]},1}^{[1]} & w_{R^{[1]},2}^{[1]} & \\cdots & w_{R^{[1]},R^{[0]}}^{[1]}\n",
    "\\end{bmatrix} \n",
    "\\hspace{1cm} \\mathbf{W}^{[2]} \\equiv \n",
    "\\begin{bmatrix}\n",
    "w_{1,1}^{[2]} & w_{1,2}^{[2]} & \\cdots & w_{1,R^{[1]}}^{[2]}\\\\\n",
    "w_{2,1}^{[2]} & w_{2,2}^{[2]} & \\cdots & w_{2,R^{[1]}}^{[2]}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{R^{[2]},1}^{[2]} & w_{R^{[2]},2}^{[2]} & \\cdots & w_{R^{[2]},R^{[1]}}^{[2]}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\mathbf{b}^{(1)} \\equiv \n",
    "\\begin{bmatrix}\n",
    "b_{1}^{[1]} \\\\\n",
    "b_{2}^{[1]} \\\\\n",
    "\\vdots  \\\\\n",
    "b_{R^{[1]}}\n",
    "\\end{bmatrix} \n",
    "\\hspace{1cm} \\mathbf{b}^{(2)} \\equiv \n",
    "\\begin{bmatrix}\n",
    "b_{1}^{[2]} \\\\\n",
    "b_{2}^{[2]} \\\\\n",
    "\\vdots  \\\\\n",
    "b_{R^{[2]}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where $R^{[i]}$ is the size of the $i^{th}$ layer ($i=0$ is the input, $i=1$ is the hidden layer, and $i=2$ is the output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569ce3d4-a014-4589-83c0-2d41b183970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_params(N_Neurons) :\n",
    "    #fix the pseudo-random number generator seed so rerunning the code produces the same output.\n",
    "    rng = np.random.default_rng(seed=123456) \n",
    "\n",
    "    #initialise the weights to a small random number between -0.015 and 0.015.\n",
    "    W1 = 0.03*rng.random((N_Neurons[1], N_Neurons[0])) -0.015\n",
    "    W2 = 0.03*rng.random((N_Neurons[2], N_Neurons[1])) -0.015\n",
    "    b1 = np.zeros((N_Neurons[1],1))\n",
    "    b2 = np.zeros((N_Neurons[2],1))\n",
    "\n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8323e-b0e8-4ee0-9530-fda34b75ebee",
   "metadata": {},
   "source": [
    "Let's test our above code for a network with three features (so the input size is three), one output, and 20 neurons in the hidden layer:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AlexSkillen/AlexSkillen.github.io/refs/heads/main/AERO40041/images/NN1.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ce8252-f342-418b-a699-2b07471b954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of W1 is:  (20, 3)\n",
      "The shape of W2 is:  (1, 20)\n",
      "The shape of b1 is:  (20, 1)\n",
      "The shape of b2 is:  (1, 1)\n"
     ]
    }
   ],
   "source": [
    "W1, W2, b1, b2 = initialise_params([3,20,1])\n",
    "print(\"The shape of W1 is: \", W1.shape)\n",
    "print(\"The shape of W2 is: \", W2.shape)\n",
    "print(\"The shape of b1 is: \", b1.shape)\n",
    "print(\"The shape of b2 is: \", b2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c043f2a-fba1-4db4-96bd-997a2fdba8ea",
   "metadata": {},
   "source": [
    "Check these are the sizes expected. Note a shape of (1, 20) means a row vector of length 20. A shape of (20, 1) is a column vector of length 20. See below:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Matrix.svg/1920px-Matrix.svg.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e8da7-ba33-44ac-8f3e-2bcafdbee0e2",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "We will use a $\\tanh$ activation function in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e438b1-f924-4c71-a6ca-8cc44f209536",
   "metadata": {},
   "source": [
    "The gradient $\\frac{d}{dx}\\tanh(x) = 1-\\tanh^2(x)$. This will be needed for backpropagation so we implement it in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3eb6b6-1f6a-497d-8fea-f22dbcd06161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_tanh(x) :\n",
    "    return 1. - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec9eec6-6975-470a-9ff6-85eb04a22ac6",
   "metadata": {},
   "source": [
    "As this is a regression task, we will use linear activation for the output layer. Linear activation means the output is equal to the input, i.e., $f(n) = n$. It's gradient is therefore just 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a7360-3630-4394-821a-135506acf0db",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "Forward propagation can be expressed as follows:\n",
    "\n",
    "$$\\mathbf{a}^{[i]} = f^{[i]}\\left(\\mathbf{W}^{[i]}\\mathbf{p}^{[i]} + \\mathbf{b}^{[i]}\\right)$$\n",
    "\n",
    "$$\\mathbf{p}^{[i]} = \n",
    "\\begin{cases}\n",
    "\\mathbf{x}_k &\\text{for the first hidden layer}\\\\\n",
    "\\mathbf{a}^{[i-1]} &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{a}^{[i]}$ is the vector of activated outputs of layer $i$, $\\mathbf{W}^{[i]}$ is the weights matrix for layer $i$, $\\mathbf{b}^{[i]}$ is the bias vector, and $\\mathbf{p}^{[i]}$ is the input to the layer (i.e. the feature vector $\\mathbf{x}_k$ if we are considering the first layer, where $k$ is the example number, or the output of the previous layer for all subsequent layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7370f7f4-9cf2-49f0-8fd1-3be94f599dfe",
   "metadata": {},
   "source": [
    "In the function below, implement the forward propagation algorithm. You will find the function `np.matmul` useful for the matrix multiplication of two matrices (or vectors), and `np.tanh` for the tanh function. Your forward function should return the vector of activation potentials and vector of activated outputs of each layer as these will be needed for backpropagation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "722aace5-decf-4621-8844-d0db08fc88c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "def forward( x, W1, W2, b1, b2 ) :\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        x (a feature vector from a single example)\n",
    "        W1 (the weights matrix for the hidden layer)\n",
    "        W2 (the weights matrix for the output layer)\n",
    "        b1 (the bias vector for the hidden layer)\n",
    "        b2 (the bias vector for the output layer)\n",
    "\n",
    "    Outputs:\n",
    "        n1 (the activation potential for the hidden layer)\n",
    "        a1 (the activated output of the hidden layer)\n",
    "        n2 (the activation potential for the output layer)\n",
    "        a2 (the activated output of the output layer)\n",
    "    \"\"\" \n",
    "    \n",
    "    n1 = np.matmul(W1, x) + b1\n",
    "    a1 = np.tanh(n1)\n",
    "    n2 = np.matmul(W2, a1) + b2\n",
    "    a2 = n2\n",
    "    return n1, a1, n2, a2\n",
    "\n",
    "AERO40041.test_forward(forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1763c-e613-4663-ab0a-52f8bd392b7a",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "We will use the MSE loss. The cost function takes the full training set and loops over all the data to compute the cost. While this function is not strictly necessary (we only need its derivative for backpropagation), it is useful to evaluate how training progresses. The function takes in X and Y arguments. X is the full training set of features where X[0] is the first feature vector, X[1] is the second, etc.\n",
    "\n",
    "Each element X[k] is the $k^{\\mathrm{th}}$ feature vector in column format. I.e. \n",
    "\n",
    "$$X[k] = \\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_D \n",
    "\\end{bmatrix}_k\n",
    "$$\n",
    "\n",
    "where $D$ is the number of features (also referred to as the dimension). \n",
    "\n",
    "The same applies for Y; Y[k] is also a vector in column format to account for the general case where the network outputs more than one value. The vector of labels Y[k] corresponds to the feature vector X[k], so X[1] is the feature vector associated with Y[1], X[100] is the feature vector associated with Y[100], etc.\n",
    "\n",
    "To keep the code general and ensure it works no matter if we have a single feature, or a single output while still using `matmul`, we store each X[k] as a (1,1) matrix for the single feature case. Similarly for Y, we store each label as a (1,1) matrix for cases of scalar output.\n",
    "\n",
    "Implement the MSE cost function below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23906e78-e7c0-4bcf-ab9e-551487f9890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "def cost(X, Y, W1, W2, b1, b2):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X (the full training set of feature vectors)\n",
    "        Y (the full training set of labels)\n",
    "        W1 (the weights matrix for the hidden layer)\n",
    "        W2 (the weights matrix for the output layer)\n",
    "        b1 (the bias vector for the hidden layer)\n",
    "        b2 (the bias vector for the output layer)\n",
    "\n",
    "    Output:\n",
    "        c (the cost)\n",
    "    \"\"\"\n",
    "    \n",
    "    c=0.\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        _, _, _, a2 = forward(X[i], W1, W2, b1, b2)\n",
    "        c = c + (Y[i] - a2)**2\n",
    "\n",
    "    c = c/float(len(X))\n",
    "\n",
    "    \"\"\"\n",
    "    In the case of more than one output, the above will compute a vector of MSE costs for each output. \n",
    "    In this example (and your coursework), since we have only one output, we could just return c calculated above\n",
    "    directly, but to deal with the more general case, we will sum the MSE associated with each output to get\n",
    "    the overall cost. This last step is optional if we always have single outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    c = np.sum(c)\n",
    "    \n",
    "    return c\n",
    "\n",
    "AERO40041.test_cost(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb00aef5-2144-4a70-b7e0-b99dcaa9b0e0",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "The backpropagation equations are given below. Here we focus on the practical implementation. The theory will be covered in the lectures.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf W^{[i]}} &=\n",
    "%\n",
    "\\left[\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}^{[i]}} \\odot f^\\prime(\\mathbf{n}^{[i]}) \\right] \\mathbf{a}^{[i-1]T}\n",
    "&\n",
    "\\text{Eq. 1}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf b^{[i]}} &=\n",
    "%\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}^{[i]}} \\odot f^\\prime(\\mathbf{n}^{[i]})\n",
    "& \\text{Eq. 2}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $\\odot$ is the Hadamard product (i.e. multiply element-wise two vectors of the same size to produce a third vector of the same size). For example \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "5 \\\\\n",
    "6 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "10 \\\\\n",
    "18 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In python, the Hadamard product can be implemented with the `*` operator. i.e. `a*b` will return $\\mathbf{a} \\odot \\mathbf{b}$ if `a` and `b` are both vectors. They can be column vectors, or row vectors (but not one of each).  \n",
    "\n",
    "The partial derivative vector $\\partial \\mathcal{L} / \\partial \\mathbf{a}^{[L]}$ for the final layer is found by differentiation of the loss function. For all previous layers, it is found as: \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}^{[L]}}\n",
    "&=\n",
    "\\mathbf{W}^{[L+1]T}\n",
    "\\left[\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}^{[L+1]}} \\odot f^\\prime(\\mathbf{n}^{[L+1]})\\right]\n",
    "& \\text{Eq. 3}\n",
    "\\end{align}\n",
    "\n",
    "Implement the backpropagation algorithm below. We will use Stochastic gradient descent, so gradients are computed with only one example. The function can take arguments `x`, `y`, `n1`, `a1`, `n2`, `a2`, `W1`, `W2`, `b1`, `b2` and `alpha` described in the template below.\n",
    "\n",
    "Values for `n1`, `a1`, `n2`, `a2` will be found from the forward propagation you implemented earlier.\n",
    "\n",
    "Your function should also update the parameters by SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd95f32-5069-45e6-bd49-e955d4c6220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This has not passed our test. This does not necessarily mean it is incorrect (for instance if you have changed the order of arguments the test might produce a false negative). It is recommended you recheck you code or ask for assistance if you cannot spot any issues.\n"
     ]
    }
   ],
   "source": [
    "def backward(x, y, n1, a1, n2, a2, W1, W2, b1, b2, alpha) :\n",
    "    \"\"\"\n",
    "        ==========================================================\n",
    "    Inputs:\n",
    "        x (a single feature vector)\n",
    "        y (a single label)\n",
    "        n1 (the activation potential for the hidden layer)\n",
    "        a1 (the activated output of the hidden layer)\n",
    "        n2 (the activation potential for the output layer)\n",
    "        a2 (the activated output of the output layer)\n",
    "        W1 (the weights matrix for the hidden layer in the data structure format as specified by initialise_params)\n",
    "        W2 (the weights matrix for the output layer in the data structure format as specified by initialise_params)\n",
    "        b1 (the bias vector for the hidden layer in the data structure format as specified by initialise_params)\n",
    "        b2 (the bias vector for the output layer in the data structure format as specified by initialise_params)\n",
    "        alpha (the learning rate)\n",
    "    Outputs:\n",
    "        W1 (the weights matrix for the hidden layer after SGD update)\n",
    "        W2 (the weights matrix for the output layer after SGD update)\n",
    "        b1 (the bias vector for the hidden layer after SGD update)\n",
    "        b2 (the bias vector for the output layer after SGD update)\n",
    "        ==========================================================\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Step 1. Compute the partial derivative of the Loss w.r.t. the output\n",
    "    Note we do not need to sum this up to get the cost function because we are using SGD without any mini-batching.\n",
    "    \"\"\"\n",
    "    \n",
    "    dL_da2 = -2 * (y - a2)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Step 2. Find the gradients of the activation functions evaluated at n. These appear in Eq. 1, 2 and 3 as f primed \n",
    "    where f prime is the gradient of the activation function. For the output layer, we use linear \n",
    "    activation so its derivative is 1. We use np.ones_like to get a vector of ones the same shape as dL_da2. \n",
    "    For the hidden layer, we call the Grad_tanh function we defined above:\n",
    "    \"\"\"\n",
    "    \n",
    "    f2_prime_n2 = np.ones_like(dL_da2)  \n",
    "    f1_prime_n1 = Grad_tanh(n1)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Step 3. Get the partial derivatives dL/da1 for the hidden layer. This is found from Eq. 3 above.\n",
    "    \"\"\"\n",
    "\n",
    "    dL_da1 = None #replace None with your implementation\n",
    "\n",
    "    \"\"\"\n",
    "    Step 4.\n",
    "    Now we have all the individual terms appearing in Eq 1, 2 and 3, we can compute dL/dW and dL/db for each layer\n",
    "    from Eq 1 and Eq2.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #From Eq. 1:\n",
    "    dL_dW2 = None #replace None with your implementation\n",
    "    dL_dW1 = None #replace None with your implementation\n",
    "\n",
    "    #from Eq. 2:\n",
    "    dL_db2 = None #replace None with your implementation\n",
    "    dL_db1 = None #replace None with your implementation\n",
    "\n",
    "    \"\"\"\n",
    "    Step 5:\n",
    "    Now we have the gradients, we apply stochastic gradient descent to update the parameters:\n",
    "    \"\"\"\n",
    "        \n",
    "    W1 = None #replace None with your implementation\n",
    "    W2 = None #replace None with your implementation\n",
    "    b1 = None #replace None with your implementation\n",
    "    b2 = None #replace None with your implementation\n",
    "\n",
    "    \"\"\"\n",
    "    Finally, we can return the updated parameters!\n",
    "    \"\"\"\n",
    "    return W1, W2, b1, b2\n",
    "    \n",
    "AERO40041.test_backward(backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71295df-562a-409a-9fd8-a8fab6d31c5a",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "In the cell below, we implement a training loop that loops over all examples in the training set in random order (this is important for SGD) and calls `forward` and then `backward` successively. This whole operation is repeated `epoch` times, where `epoch` is an argument to the function that defaults to 10, but can be set to any value when the function is called. \n",
    "\n",
    "The line `np.arange(len(X))` outputs a vector $[0, 1, 2, 3 \\cdots (N-1)]$. This vector is rearranged into a random order with the line `np.random.shuffle(indices)`. The data is then copied into X_shuff and Y_shuff in this random order. We then loop over the shuffled data calling our forward and backward functions on a single example (i.e. SGD, no mini-batching).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "140177d5-d66c-4e2d-813f-b04af7ca1efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, W1, W2, b1, b2, alpha = 0.01, epoch = 10):\n",
    "    for j in range(epoch):\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuff = X[indices]\n",
    "        Y_shuff = Y[indices]\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            n1, a1, n2, a2 = forward(X_shuff[i], W1, W2, b1, b2)\n",
    "            W1, W2, b1, b2 = backward(X_shuff[i], Y_shuff[i], n1, a1, n2, a2, W1, W2, b1, b2, alpha)\n",
    "        if(j%100 == 0) :\n",
    "            print(\"epochs:\", j + 1, \"======== Cost:\", cost(X, Y, W1, W2, b1, b2))  \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763b857-564c-435f-a04b-accd06bb3d95",
   "metadata": {},
   "source": [
    "### Putting it all together.\n",
    "Now we have defined all the functions we need. Next, we will use them to build a neural network that takes a single feature, has one hidden layer containing 50 neurons, and then outputs a scalar. \n",
    "\n",
    "We will test this model on a toy dataset of $y=\\sin(2\\pi x)$. First, `np.linspace` is used to make $500$ equally spaced values between 0 and 1, which we will use as our features (500 samples in the dataset, with a single feature per each sample). The line `Y = np.sin(X*2.*np.pi)` then computes the corresponding labels. \n",
    "\n",
    "Remember that since we wanted a general code that works with `matmul`, even though we only have one feature and a scalar label, we need each $x$ and each $y$ to be stored as a $(1\\times1)$ matrix. The lines `X=X.reshape(-1,1,1)` and `Y=Y.reshape(-1,1,1)` reshape our data accordingly. The $-1$ means we do not explicitly set the first dimension (which will equal $N$ where $N=500$ in this case) but let numpy work it out based on how many numbers the array holds, and the constraints that each one should be a $(1\\times1)$ matrix. We could equally have put `X=X.reshape(500,1,1)`, but then we would need to update the code every time our training set size changes. Run the cell below to see if your implementation worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd1d551-7520-4fab-b8a4-dd0cae9558d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type NoneType which has no callable tanh method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'tanh'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m X\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m Y\u001b[38;5;241m=\u001b[39mY\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_layer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_layer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_layer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_layer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(X, Y, W1, W2, b1, b2, alpha, epoch)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)):\n\u001b[0;32m      9\u001b[0m     n1, a1, n2, a2 \u001b[38;5;241m=\u001b[39m forward(X_shuff[i], W1, W2, b1, b2)\n\u001b[1;32m---> 10\u001b[0m     W1, W2, b1, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_shuff\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_shuff\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(j\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) :\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======== Cost:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cost(X, Y, W1, W2, b1, b2))  \n",
      "Cell \u001b[1;32mIn[7], line 40\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(x, y, n1, a1, n2, a2, W1, W2, b1, b2, alpha)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mStep 2. Find the gradients of the activation functions evaluated at n. These appear in Eq. 1, 2 and 3 as f primed \u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03mwhere f prime is the gradient of the activation function. For the output layer, we use linear \u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03mactivation so its derivative is 1. We use np.ones_like to get a vector of ones the same shape as dL_da2. \u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03mFor the hidden layer, we call the Grad_tanh function we defined above:\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m f2_prime_n2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(dL_da2)  \n\u001b[1;32m---> 40\u001b[0m f1_prime_n1 \u001b[38;5;241m=\u001b[39m \u001b[43mGrad_tanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03mStep 3. Get the partial derivatives dL/da1 for the hidden layer. This is found from Eq. 3 above.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m dL_da1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m#replace None with your implementation\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m, in \u001b[0;36mGrad_tanh\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGrad_tanh\u001b[39m(x) :\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type NoneType which has no callable tanh method"
     ]
    }
   ],
   "source": [
    "N_Neurons = [1, 50, 1] #number of neurons in each layer (input, hidden and output)\n",
    "\n",
    "W_layer1, W_layer2, b_layer1, b_layer2 = initialise_params( N_Neurons )\n",
    "\n",
    "np.random.seed(123456)\n",
    "\n",
    "#prepare some toy data (y=sin(2*pi * x))\n",
    "X=np.linspace(0,1,500)\n",
    "Y = np.sin(X*2.*np.pi)\n",
    "X=X.reshape(-1,1,1)\n",
    "Y=Y.reshape(-1,1,1)\n",
    "\n",
    "train(X,Y, W_layer1, W_layer2, b_layer1, b_layer2, alpha=0.001, epoch=6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02694a89-3762-481a-b78b-9a38af277615",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "Run the cell below to plot the target and prediction. How did you do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a44a79-1446-4382-a69e-cf155287ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.reshape(-1,1),Y.reshape(-1,1), label='target')\n",
    "\n",
    "pred = np.zeros_like(Y)\n",
    "for i in range(len(X)):\n",
    "    _, _, _, pred[i] = forward(X[i], W_layer1, W_layer2, b_layer1, b_layer2)\n",
    "\n",
    "plt.plot(X.reshape(-1,1),pred.reshape(-1,1), label='prediction')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870988cf-f3c8-40c8-af92-a114fec927ad",
   "metadata": {},
   "source": [
    "### Things to try:\n",
    "\n",
    "1. Play with the hyperparameters of your network. Does the prediction get worse? I pre-populated the cells with hyperparameters that work quite well after a brief hyperparameter search, but maybe you can get better. Have a play.\n",
    "\n",
    "2. Try to get your network to predict a different function.\n",
    "\n",
    "3. Can you implement a deeper network with 2 hidden layers?\n",
    "\n",
    "4. [Advanced - only recommended if you want a challenge!] Can you implement a deeper network with an arbitrary number of hidden layers? The code should work with `N_Neurons = [a, b, c, d, e]` or `N_Neurons = [a, b, c, d, e, f, g]` without any modification of the code (other than the line that specifies the network architecture, of course!). Your code should work for `a`, `b` etc. as any integer value.\n",
    "\n",
    "5. Edit the implementation from a regression task to a classification task. Hint: You may use the sigmoid function for the output layer and interpret the output as a probability of the class being that associated with label 1. (This is your coursework assignment).\n",
    "\n",
    "\n",
    "A common \"trick\" in the ML community is to implement sigmoid as in the cell below. This helps prevent numerical overflow over the naive implementation of $\\sigma(z) = 1 / (1 + e^{(-z)})$, but gives the same result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7a1be-120a-4c51-acfd-a4989eea0827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_with_trick(z):\n",
    "    if( z>=0. ):\n",
    "        return 1. / (1. + np.exp(-z))\n",
    "    else:\n",
    "        return np.exp(z) / (1. + np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3998f10c-3553-45f3-a82b-8a6f97f9793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e6396-cdb9-42ea-986f-7d18a7ca30e0",
   "metadata": {},
   "source": [
    "For example, try running the two cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1842d67-31c9-4978-9823-24df3070cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_sigmoid(-1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452aa2e-73d6-40f5-b46f-4cbf224fce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_with_trick(-1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
